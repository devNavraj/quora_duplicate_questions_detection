{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a38ee21-6cb4-44af-9d9c-819d905e7aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV, ShuffleSplit\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score \n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, log_loss\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import xgboost as xgb\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d02300bd-f1c2-4a22-9cb9-82c7bd6782a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "traindata = pd.read_csv(\"data/train.csv\")\n",
    "testdata = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "916a1b6d-61c4-4f0d-9b99-82d75a8339a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404287, 6)\n"
     ]
    }
   ],
   "source": [
    "traindata = traindata.dropna()\n",
    "print(traindata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d76a2b21-46dd-4a22-b7f8-3dc8732b81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the NLP features\n",
    "train_data = pd.read_csv(\"data/extended_nlp_features_train.csv\")\n",
    "test_data = pd.read_csv(\"data/extended_nlp_features_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a75f2428-1c65-45bb-b0ec-67ed797e31b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining cosine similarity and euclidean distance between two vectors\n",
    "def cosine_euclidean(u, v):\n",
    "    return np.array([np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v)), np.linalg.norm(u - v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7c4258e-a0da-4e98-8209-d5221e93792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open .npy files and loop through the sentence embeddings for train data\n",
    "with open('temp_train_question1_sentenceBERT.npy', 'rb') as q1_vec, open('temp_train_question2_sentenceBERT.npy', 'rb') as q2_vec:\n",
    "    distances = []\n",
    "    while True:\n",
    "        try:\n",
    "            q1_20k = np.load(q1_vec, allow_pickle=True)\n",
    "            q2_20k = np.load(q2_vec, allow_pickle=True)\n",
    "            for q1,q2 in zip(q1_20k, q2_20k):\n",
    "                dists = cosine_euclidean(q1, q2)\n",
    "                distances.append(dists)\n",
    "        except IOError as e:\n",
    "            distances = np.array(distances)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05a438ea-f865-4934-bb1c-f0418a015b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pd.DataFrame(distances, columns=['cosine_simlarity_bert', 'euclidean_distance_bert'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3887d5f-31b8-403d-88a1-2b474911ee21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404287, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1f9dec2-037c-458b-9071-d4a1a2613b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([train_data, pd.DataFrame(distances)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2757b08d-0b9e-493c-9e96-23b7f9efe76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404287, 30)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f64067fd-494c-4607-961c-c86b713e04ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open .npy files and loop through the sentence embeddings for test data\n",
    "with open('temp_test_question1_sentenceBERT.npy', 'rb') as q1_vec, open('temp_test_question2_sentenceBERT.npy', 'rb') as q2_vec:\n",
    "    distances = []\n",
    "    while True:\n",
    "        try:\n",
    "            q1_20k = np.load(q1_vec, allow_pickle=True)\n",
    "            q2_20k = np.load(q2_vec, allow_pickle=True)\n",
    "            for q1,q2 in zip(q1_20k, q2_20k):\n",
    "                dists = cosine_euclidean(q1, q2)\n",
    "                distances.append(dists)\n",
    "        except IOError as e:\n",
    "            distances = np.array(distances)\n",
    "            break\n",
    "distances = pd.DataFrame(distances, columns=['cosine_simlarity_bert', 'euclidean_distance_bert'])\n",
    "test_data = pd.concat([test_data, pd.DataFrame(distances)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ff7f38d-aa42-4b6a-ac1c-d8c7a1a49b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(columns=['question1_final', 'question2_final'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7eb2ebc5-2e7e-4597-af47-f7c74bc41ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404287, 28)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7085535-d360-4d81-b253-52943434f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test data\n",
    "test_data.drop(columns=['question1_final', 'question2_final'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b9c72f1-6d3f-4ace-8ef7-69c6af4483fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"data/train_features.csv\", index=False)\n",
    "test_data.to_csv(\"data/test_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385dccc3-9a2f-46aa-bc0f-762df392f7ce",
   "metadata": {},
   "source": [
    "## Data Preparation for Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbc71534-97ff-4aea-951b-8fabf898fc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "451cc6c7-4756-4ac0-809d-d48a4890f49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404287, 28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6fd70f2-8f04-4d0e-8d3b-2f03ef8ed8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34514dfe-5125-49cb-b896-78409e5f7310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404287, 28)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit_transform(train_data)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8c2063c-72af-448e-b81d-63f325d60180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09401709, 0.06041479, 0.0859375 , ..., 0.25124378, 0.62265475,\n",
       "        0.59195244],\n",
       "       [0.10940171, 0.03697024, 0.109375  , ..., 0.40677565, 0.79336938,\n",
       "        0.4548728 ],\n",
       "       [0.1008547 , 0.02524797, 0.109375  , ..., 0.41585178, 0.83977757,\n",
       "        0.40547467],\n",
       "       ...,\n",
       "       [0.1042735 , 0.04328224, 0.078125  , ..., 0.28713575, 0.52086247,\n",
       "        0.69004651],\n",
       "       [0.18974359, 0.11000902, 0.15625   , ..., 0.40378465, 0.93764583,\n",
       "        0.23590894],\n",
       "       [0.0974359 , 0.03967538, 0.0625    , ..., 0.46898839, 0.75761155,\n",
       "        0.48530176]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For test data\n",
    "test_data = test_data.to_numpy()\n",
    "test_data = scaler.transform(test_data)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8a95cc0-adc0-4bc9-919f-4e18f0160ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp_testdata.npy', 'wb') as f:\n",
    "    batch = 20000\n",
    "    while(len(test_data)):\n",
    "        tempdata = test_data[:batch]\n",
    "        test_data = test_data[batch:]\n",
    "        np.save(f, tempdata, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "726c3d50-75a9-4b34-82d4-eb13df509d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadVectors(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        q_vectors = []\n",
    "        while True:\n",
    "            try:\n",
    "                q_vec = np.load(f, allow_pickle=True)\n",
    "                q_vectors.extend(list(q_vec))\n",
    "            except IOError as e:\n",
    "                q_vectors = np.array(q_vectors)\n",
    "                break\n",
    "    return q_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a9fd6bb-2ace-4081-9492-f2bcadfedec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_question1_vec = loadVectors('temp_train_question1_sentenceBERT.npy')\n",
    "train_question2_vec = loadVectors('temp_train_question2_sentenceBERT.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e476efd-1adc-4dd8-bba4-5964fbb3b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.hstack((train_data, train_question1_vec, train_question2_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "811ce74c-7800-4ced-99ce-041f852acf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404287, 1564)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4004543-1bc1-4d11-ba5e-cdfcedf2acf6",
   "metadata": {},
   "source": [
    "We have 1564 features (28 + 768 + 768).\n",
    "- 28 are extracted features.\n",
    "- 768+768 for sentence embedding of question 1 and question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2034d474-faf6-4ae3-a44f-c4aca403edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_data.npy', 'wb') as f:\n",
    "    np.save(f, train_data, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0782ea-5753-40d6-9878-3d67c70d3021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
